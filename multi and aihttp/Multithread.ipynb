{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as BS\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from zipfile import ZipFile, ZIP_DEFLATED\n",
    "from multiprocessing.dummy import Pool as ThreadPool\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "from multiprocessing import Pool\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "proxy_url = '45.81.76.252'\n",
    "proxy_port = '8000'\n",
    "\n",
    "proxy = {\n",
    "    'https': f'http://{proxy_url}:{proxy_port}'\n",
    "}\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/111.0',\n",
    "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8'\n",
    "}\n",
    "\n",
    "params = {\n",
    "    'searchString': '',\n",
    "    'morphology': 'on',\n",
    "    'search-filter': 'Дате обновления',\n",
    "    'savedSearchSettingsIdHidden': '',\n",
    "    'fz94': 'on',\n",
    "    'fz223': 'on',\n",
    "    'published': 'on',\n",
    "    'regarded': 'on',\n",
    "    'considered': 'on',\n",
    "    'decisionOnTheComplaintTypeResult': '',\n",
    "    'receiptDateStart': '',\n",
    "    'receiptDateEnd': '',\n",
    "    'updateDateFrom': '',\n",
    "    'updateDateTo': '',\n",
    "    'sortBy': 'UPDATE_DATE',\n",
    "    'pageNumber': '',\n",
    "    'sortDirection': 'false',\n",
    "    'recordsPerPage': '_10',\n",
    "    'showLotsInfoHidden': 'false',\n",
    "}\n",
    "\n",
    "url = \"\"\"\n",
    "\n",
    "https://zakupki.gov.ru/epz/complaint/search/search_eis.html\n",
    "\n",
    "\"\"\".strip()\n",
    "host = '/'.join(url.split('/', maxsplit=3)[:3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_links_in_page(text: str) -> list[str]:\n",
    "    # Получам ссылки с каждой страницы (всего 10)\n",
    "    out_list = list()\n",
    "    soup = BS(text, 'html.parser')\n",
    "    blocks_list = soup.find('div', class_ = 'search-registry-entrys-block')\n",
    "    blocks_list = blocks_list.find_all('div', class_ = 'search-registry-entry-block')\n",
    "\n",
    "    for block in blocks_list:\n",
    "        href = block.find('div', class_ = 'd-flex registry-entry__header-mid align-items-center')\n",
    "        href = host + href.find('span').find('a').get('href')\n",
    "        out_list.append(href)\n",
    "    \n",
    "    return out_list\n",
    "\n",
    "def _parse_links_in_site() -> list[str]:\n",
    "    # Проходим по страницам поиска из реестра жалоб\n",
    "    # всего страниц 100\n",
    "    # на каждой странице 10 жалоб\n",
    "    # на выходе получаем список ссылок на страницу с жалобой\n",
    "    out_list = list()\n",
    "    \n",
    "    with tqdm(total=100 ) as pbar:\n",
    "        for page in range(1,101):\n",
    "            params['pageNumber'] = f'{page}'\n",
    "            response = requests.get(url=url,params=params , headers=headers, proxies=proxy)\n",
    "            out_list.append(_parse_links_in_page(response.text))\n",
    "            pbar.update(1)\n",
    "    \n",
    "    return sum(out_list, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:06<00:00,  1.49it/s]\n"
     ]
    }
   ],
   "source": [
    "links_list = _parse_links_in_site()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сохранение линков в json формате\n",
    "#--------------------------------------------\n",
    "# with open('data/links_list.json', 'w', encoding='utf-8') as file:\n",
    "#     tmp = json.dumps(links_list, indent=4, ensure_ascii=False)\n",
    "#     file.write(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка линков из json\n",
    "# --------------------------------------------\n",
    "with open('data/links_list.json', 'r', encoding='utf-8') as file:\n",
    "    tmp = file.read()\n",
    "    links_list = json.loads(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://zakupki.gov.ru/epz/complaint/card/complaint-information.html?id=2258395'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links_list[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_info_complaint(text: str):\n",
    "    soup = BS(text, 'html.parser')\n",
    "    complaint = soup.find('div', class_='sectionMainInfo borderRight col-9')\n",
    "\n",
    "    number_complaint = complaint.find('div', class_='cardMainInfo__status').find('a').get_text()[2:]\n",
    "\n",
    "    status_complaint = complaint.find('div', class_='cardMainInfo__status').find('span', class_ = 'cardMainInfo__state').get_text()\n",
    "\n",
    "    comments_complaint = list()\n",
    "    try:\n",
    "        tmp = complaint.find('span', class_='cardMainInfo__title distancedText float-left').find('span').get_text().strip()\n",
    "    except AttributeError:\n",
    "        tmp = ''\n",
    "    comments_complaint.append(tmp)\n",
    "    try:\n",
    "        tmp = complaint.find('span', class_='cardMainInfo__title distancedText float-left').find_next_sibling('span', class_='cardMainInfo__title').get_text().strip()\n",
    "    except AttributeError:\n",
    "        tmp = ''\n",
    "    comments_complaint.append(tmp)\n",
    "\n",
    "    card_common = soup.find('div', class_='card-common')\n",
    "    date_complaint = card_common.find(string='Дата размещения жалобы').find_parent().find_next_sibling('div', class_=\"common-text__value\").get_text().strip()\n",
    "    print(number_complaint)\n",
    "\n",
    "\n",
    "def _get_number(text: str) -> str:\n",
    "    soup = BS(text, 'html.parser')\n",
    "    complaint = soup.find('div', class_='sectionMainInfo borderRight col-9')\n",
    "    number_complaint = complaint.find('div', class_='cardMainInfo__status').find('a').get_text()[2:]\n",
    "    return number_complaint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Однопоточная загрузка страниц\n",
    "\n",
    "# def load_pages(links_list: list[str]) -> dict[str, str]:\n",
    "#     with tqdm(total=len(links_list)) as qbar:\n",
    "#         out_dict = dict()\n",
    "#         for link in links_list:\n",
    "#             response = requests.get(url=link, headers=headers, proxies=proxy)\n",
    "#             number = _get_number(response.text)\n",
    "#             out_dict[number] = response.text\n",
    "#             qbar.update(1)\n",
    "#         return out_dict\n",
    "\n",
    "# def save_pages(in_dict: dict[str, str]) -> None:\n",
    "#     with ZipFile('data/pages.zip', 'w', compression=ZIP_DEFLATED, compresslevel=1) as zip_file:\n",
    "#         for name, value in in_dict.items():\n",
    "#             zip_file.writestr(f'{name}.html', value)\n",
    "            \n",
    "# pages_dict = load_pages(links_list=links_list)\n",
    "\n",
    "# save_pages(pages_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_page(input_tuple: tuple[str, tqdm]):\n",
    "    link, pbar = input_tuple\n",
    "    response = requests.get(url=link, headers=headers, proxies=proxy)\n",
    "    pbar.update(1)\n",
    "    return response.text\n",
    "\n",
    "def run_scraping_multitreading(input_link_list: list[str], processes: int = 8):\n",
    "    # start = time.time()\n",
    "    lenght = len(input_link_list)\n",
    "    with tqdm(total=lenght) as pbar:\n",
    "        task_list = list(zip(input_link_list, [pbar]*lenght))\n",
    "        with ThreadPool(processes=processes) as pool:\n",
    "            workerreturn = pool.map(_get_page, task_list)\n",
    "    # delta = time.time() - start\n",
    "    # return (workerreturn, delta)\n",
    "    return workerreturn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Тестовый вариант, что бы определить количество потоков и скорость с которой они работуют\n",
    "# filename = 'data/Thread_test.csv'\n",
    "# if os.path.isfile(filename):\n",
    "#     df = pd.read_csv(filename)\n",
    "# else:\n",
    "#     df = pd.DataFrame()\n",
    "\n",
    "# n = 8\n",
    "# try:\n",
    "#     for _ in range(4):\n",
    "#         page, delta_time = run_scraping_multitreading(links_list, n)\n",
    "#         df_temp = pd.DataFrame(\n",
    "#             {\n",
    "#                 'Count_Thread': [n],\n",
    "#                 'Time': delta_time\n",
    "#             }\n",
    "#         )\n",
    "#         df = pd.concat([df, df_temp], ignore_index=True, sort=False)\n",
    "        \n",
    "# except Exception as ex:\n",
    "#     print(ex)\n",
    "# finally:\n",
    "#     df.to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:45<00:00, 21.96it/s]\n"
     ]
    }
   ],
   "source": [
    "pages = run_scraping_multitreading(links_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with ZipFile('data/pages.zip') as zip_file:\n",
    "    pages = list()\n",
    "    for item in zip_file.filelist:\n",
    "        with zip_file.open(item.filename) as file:\n",
    "            pages.append(file.read())\n",
    "\n",
    "def _get_number_th(lst:list[str]):\n",
    "    with ThreadPool(16) as pool:\n",
    "        out = pool.map(_get_number, lst)\n",
    "        return out\n",
    "    \n",
    "if __name__ =='__main__':\n",
    "    with ZipFile('data/pages_test.zip', 'w', compression=ZIP_DEFLATED, compresslevel=1) as zip_file:\n",
    "        numbers = _get_number_th(pages)\n",
    "        for number, page in zip(numbers, pages):\n",
    "        # for number, page in enumerate(pages):\n",
    "            zip_file.writestr(f'{number}.html', page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'4.972095966339111\\r\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args = ['python', 'multi.py']\n",
    "subprocess.run(args=args, check=True, stdout=subprocess.PIPE).stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['4.978644609451294']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = !python multi.py\n",
    "# result = json.loads(result)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.978644609451294"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "json.loads(result[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
