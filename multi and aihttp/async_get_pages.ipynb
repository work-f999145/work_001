{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as BS\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from zipfile import ZipFile, ZIP_DEFLATED\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "import aiohttp, asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "proxy_url = '45.81.76.252'\n",
    "proxy_port = '8000'\n",
    "\n",
    "proxy = {\n",
    "    'https': f'http://{proxy_url}:{proxy_port}'\n",
    "}\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/111.0',\n",
    "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8'\n",
    "}\n",
    "\n",
    "params = {\n",
    "    'searchString': '',\n",
    "    'morphology': 'on',\n",
    "    'search-filter': 'Дате обновления',\n",
    "    'savedSearchSettingsIdHidden': '',\n",
    "    'fz94': 'on',\n",
    "    'fz223': 'on',\n",
    "    'published': 'on',\n",
    "    'regarded': 'on',\n",
    "    'considered': 'on',\n",
    "    'decisionOnTheComplaintTypeResult': '',\n",
    "    'receiptDateStart': '',\n",
    "    'receiptDateEnd': '',\n",
    "    'updateDateFrom': '',\n",
    "    'updateDateTo': '',\n",
    "    'sortBy': 'UPDATE_DATE',\n",
    "    'pageNumber': '',\n",
    "    'sortDirection': 'false',\n",
    "    'recordsPerPage': '_10',\n",
    "    'showLotsInfoHidden': 'false',\n",
    "}\n",
    "\n",
    "url = \"\"\"\n",
    "\n",
    "https://zakupki.gov.ru/epz/complaint/search/search_eis.html\n",
    "\n",
    "\"\"\".strip()\n",
    "host = '/'.join(url.split('/', maxsplit=3)[:3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_links_in_page(text: str) -> list[str]:\n",
    "    # Получам ссылки с каждой страницы (всего 10)\n",
    "    out_list = list()\n",
    "    soup = BS(text, 'html.parser')\n",
    "    blocks_list = soup.find('div', class_ = 'search-registry-entrys-block')\n",
    "    blocks_list = blocks_list.find_all('div', class_ = 'search-registry-entry-block')\n",
    "\n",
    "    for block in blocks_list:\n",
    "        href = block.find('div', class_ = 'd-flex registry-entry__header-mid align-items-center')\n",
    "        href = host + href.find('span').find('a').get('href')\n",
    "        out_list.append(href)\n",
    "    \n",
    "    return out_list\n",
    "\n",
    "def _parse_links_in_site() -> list[str]:\n",
    "    # Проходим по страницам поиска из реестра жалоб\n",
    "    # всего страниц 100\n",
    "    # на каждой странице 10 жалоб\n",
    "    # на выходе получаем список ссылок на страницу с жалобой\n",
    "    out_list = list()\n",
    "    \n",
    "    with tqdm(total=100 ) as pbar:\n",
    "        for page in range(1,101):\n",
    "            params['pageNumber'] = f'{page}'\n",
    "            response = requests.get(url=url,params=params , headers=headers, proxies=proxy)\n",
    "            out_list.append(_parse_links_in_page(response.text))\n",
    "            pbar.update(1)\n",
    "    \n",
    "    return sum(out_list, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# links_list = _parse_links_in_site()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сохранение линков в json формате\n",
    "#--------------------------------------------\n",
    "# with open('data/links_list.json', 'w', encoding='utf-8') as file:\n",
    "#     tmp = json.dumps(links_list, indent=4, ensure_ascii=False)\n",
    "#     file.write(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Загрузка линков из json\n",
    "# # --------------------------------------------\n",
    "# with open('data/links_list.json', 'r', encoding='utf-8') as file:\n",
    "#     tmp = file.read()\n",
    "#     links_list = json.loads(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def _get_pages_async(\n",
    "                            session: aiohttp.ClientSession,\n",
    "                            link: str\n",
    "    ):\n",
    "    \n",
    "    async with session.get(url=link, proxy=proxy.get('https', '')) as response:\n",
    "        response_text = await response.text()\n",
    "    return response_text\n",
    "\n",
    "async def _get_pages_tasks(\n",
    "                            input_links_list: list[str]\n",
    "    ):\n",
    "    \n",
    "    tasks = list()\n",
    "    async with aiohttp.ClientSession(headers=headers) as session:\n",
    "        for link in input_links_list:\n",
    "            task = asyncio.create_task(_get_pages_async(session=session, link=link))\n",
    "            tasks.append(task)\n",
    "            \n",
    "        pages = await asyncio.gather(*tasks)\n",
    "    return pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "test = await _get_pages_tasks(links_list)\n",
    "print(time.time() -  start)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
